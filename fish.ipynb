{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "torch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределите в команде гпу, задайте свой номер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuda_device = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем полезную либу `attrdict`. Чем она хороша: позволяет обращаться к элементам словаря, как к его атрибутам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from attrdict import AttrDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с аудио и текстом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В распознавании речи нейронная сеть обучается на парах аудио+текст.\n",
    "\n",
    "Давайте научимся открывать аудиофайлы и подготавливать их для работы с нейронной сетью.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "На лекциях мы обсуждали, что аудио может быть записано с разной частотой дискретизации (sample rate), но для обучения нейронной сети обычно все аудио приводят к одной частоте дискретизации. (В этом проекте мы будем использовать sample rate 8000).\n",
    "\n",
    "Так же аудио может быть одноканальным или многоканальным. Проблема с многоканальным аудио может быть в том, что речь в двух каналах может звучать одновременно, такие аудио надо разделять на 2 дорожки. В нашем же случае данные подобраны таким образом, что такой ситуации не возникет, поэтому несколько каналов можно просто усреднить.\n",
    "\n",
    "\n",
    "Будем использовать библиотеку `torchaudio` [docs](https://pytorch.org/audio/).\n",
    "\n",
    "Реализуйте функцию `open_audio`, которая открывает аудио (искать [тут](https://pytorch.org/audio/stable/torchaudio.html)), усредняет аудио по всем каналам (это можно сделать обычным усреднением) и приводит к необходимой частоте дискретизации (искать [тут](https://pytorch.org/audio/stable/transforms.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "torchaudio"
    ]
   },
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "open_audio"
    ]
   },
   "outputs": [],
   "source": [
    "def open_audio(audio_path, desired_sample_rate):\n",
    "    \n",
    "    \"\"\" Open and resample audio, average across channels\n",
    "        Inputs:\n",
    "            audio_path: str, path to audio\n",
    "            desired_sample_rate: int, the sampling rate to which we would like to convert the audio\n",
    "        Returns:\n",
    "            audio: 1D tensor with shape (num_timesteps)\n",
    "            audio_len: int, len of audio\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ### write your code here ###\n",
    "    \n",
    "    return audio, audio_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите тесты, чтобы проверить себя.\n",
    "<img src=\"images/tests_are_all_we_need.png\" width=\"400\" height=\"600\">\n",
    "\n",
    "\n",
    "**Перед каждым запуском тестов не забывайте сохранять ноутбук.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_open_audio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте откроем аудио, и послушаем, что у нас получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_rate=8000\n",
    "audio, audio_len = open_audio('test_files/test_audio.mp3', sample_rate)\n",
    "Audio(data=audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же послушать аудио можно через путь к аудио файлу.\n",
    "\n",
    "Можно заметить, что звучание немного поменялось. Это произошло из-за того, что мы поменяли оригинальный sample rate 48000Hz на 8000Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio('test_files/test_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech2text  —  это не только speech, но и text, поэтому теперь давайте поговорим о предобработке текста. \n",
    "\n",
    "Первым шагом необходимо привести текст к нижнему регистру и удалить пунктуацию. Если на этом этапе в тексте содержатся символы кроме русского алфавита и пробела (например, цифры), то такие примеры лучше вообще убрать из обучающей выборки (если просто убрать символ из строки, то может нарушиться соответствие аудио-текст).\n",
    "\n",
    "В размеченных данных для обучения и валидации уже произедена очистка и удаление ненужных символов, поэтому нам надо только извлечь токены из текста.\n",
    "\n",
    "В качестве токенов для обучения нейронной сети будем использовать буквы русского алфавита и пробел. Так же, как было сказано в лекциях, нам потребуется специальный символ `<blank>` для построения выравниваний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "alphabet"
    ]
   },
   "outputs": [],
   "source": [
    "alphabet = ['а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к',\n",
    "            'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц',\n",
    "            'ч', 'ш', 'щ', 'ь', 'ы', 'ъ', 'э', 'ю', 'я',\n",
    "            ' ', '<blank>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но, естественно, в таком виде передавать буквы в нейронную сеть мы не можем — их нужно закодировать в числа. Для этого будем использовать уже готовую функцию Vocab.\n",
    "\n",
    "Обратите внимание, что Vocab добавляет дополнительный токен `<unk>`, ответственный за все символы, которых нет в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "vocab"
    ]
   },
   "outputs": [],
   "source": [
    "from vocabulary import Vocab\n",
    "vocab = Vocab(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попрактикуйтесь с Vocab. Для того, чтобы понять все возможности Vocab можно заглянуть в код vocabulary.py и почитать докстринги.\n",
    "\n",
    "* переведите \"привет\" в индексы\n",
    "* переведите [11, 0, 11, 33, 4, 5, 12, 0] в текст\n",
    "* что будет если перевести в индексы слово \"hi\"?\n",
    "* а [44, 5] в текст?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие еще у Vocab возможности?\n",
    "\n",
    "Если в процессе исследования вы изменили текущий vocab (путем добавления нового токена, например), не забудьте вернуть vocab к начальному состоянию  `vocab = Vocab(alphabet)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте извлечем из Vocab и сохраним в переменные два важных значения - длину алфавита и значения индекса `<blank>`.\n",
    "    \n",
    "Эти значения нам еще много раз пригодятся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "get_num_tokens",
     "get_blank_index"
    ]
   },
   "outputs": [],
   "source": [
    "def get_num_tokens(vocab):\n",
    "    ### write your code here ###\n",
    "    return num_tokens\n",
    "\n",
    "def get_blank_index(vocab):\n",
    "    ### write your code here ###\n",
    "    return blank_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_num_tokens.py\n",
    "! pytest tests/test_blank_index.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_tokens = get_num_tokens(vocab) \n",
    "blank_index = get_blank_index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать нейронную сеть мы будем на подготовленном датасете. Для удобства мы сгруппировали обучающие и валидационные данные в текстовые файлы, где в каждой строке через запятую указан путь к файлу, соответствующий текст, длина аудио (в секундах).\n",
    "\n",
    "\n",
    "Давайте реализуем функцию AudioDataset, которая подготавливает текст и аудио для каждого элемента датастета.\n",
    "\n",
    "\n",
    "На этих данных мы будем обучать модель с помощью ctc лосса, а мы уже знаем, что для обучения с ctc аудио лучше отсортировать по длине - так модели будет проще обучиться.\n",
    "\n",
    "Изучить документацию по Dataset и DataLoader (пригодится далее) можно найти [тут](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path,\n",
    "                 vocab,\n",
    "                 sample_rate=8000,\n",
    "                 ):\n",
    "        self.vocab = vocab\n",
    "        self.sample_rate = sample_rate\n",
    "        data = pd.read_csv(dataset_path, header=None, names=['audio_path', 'text', 'duration'])\n",
    "        data['duration'] = data['duration'].astype(float)\n",
    "        self.data = data.sort_values(by='duration')\n",
    "\n",
    "    def __len__(self):\n",
    "        return # YOUR CODE\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.data.audio_path[idx]\n",
    "        text = self.data.text[idx]\n",
    "\n",
    "        \n",
    "        ### write your code here ###\n",
    "        \n",
    "        return {\"audio\":  audio,  # torch tensor, (num_timesteps)\n",
    "                \"audio_len\": audio_len, # int\n",
    "                \"text\": text, # str\n",
    "                \"text_len\": text_len, # int\n",
    "                'tokens': tokens, # torch tensor, (text_len)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как будут выглядеть элементы полученного датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = '/data/manifests/train.txt'\n",
    "val_dataset = '/data/manifests/val.txt'\n",
    "test_dataset = '/data/manifests/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element number: 0\n",
      "{'audio': tensor([ 0.0000,  0.0000,  0.0000,  ...,  0.0005, -0.0008, -0.0006]),\n",
      " 'audio_len': 8360,\n",
      " 'text': 'один',\n",
      " 'text_len': 4,\n",
      " 'tokens': tensor([15,  4,  9, 14])}\n",
      "\n",
      "element number: 1\n",
      "{'audio': tensor([-1.3758e-09, -2.2855e-09, -1.3640e-09,  ..., -2.1114e-04,\n",
      "        -4.1249e-04, -2.6402e-04]),\n",
      " 'audio_len': 8640,\n",
      " 'text': 'ноль',\n",
      " 'text_len': 4,\n",
      " 'tokens': tensor([14, 15, 12, 27])}\n",
      "\n",
      "element number: 2\n",
      "{'audio': tensor([0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0080, 0.0038]),\n",
      " 'audio_len': 8987,\n",
      " 'text': 'два',\n",
      " 'text_len': 3,\n",
      " 'tokens': tensor([4, 2, 0])}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = AudioDataset(train_dataset, vocab)\n",
    "\n",
    "for i, element in enumerate(dataset):\n",
    "    print(f'element number: {i}')\n",
    "    pprint(element)\n",
    "    print()\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы эффективно обучать нейронную сеть, необходимо подавать в нее данные батчами. В этом нам поможет функция `torch.utils.data.DataLoader`. \n",
    "\n",
    "Обратите внимание, что некоторые данные в датасете разной длины (например, `audio`), для формирования батча из таких данных  необходимо использовать паддинг. Для этого можно реализовать фукцию `collate_fn`. Подробнее про то, как именно использовать `collate_fn` можно почитатать в доках к `torch.utils.data.DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\" \n",
    "        Inputs:\n",
    "            batch: list of elements with length=batch_ize\n",
    "        Returns:\n",
    "            dict\n",
    "    \"\"\"\n",
    "    \n",
    "    ### write your code here ###\n",
    "\n",
    "    return {'audios': audios, # torch tensor, (batch_size, max_num_timesteps)\n",
    "            'audio_lens': audio_lens, # torch tensor, (batch_size)\n",
    "            'texts': texts,  # list, len=(batch_size)\n",
    "            \"text_lens\": text_lens, # torch tensor, (batch_size)\n",
    "            'tokens': tokens,  # torch tensor, (batch_size, max_text_len)\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_collate_fn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 90\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании DataLoader не забудьте использовать параметры `batch_size`, `num_workers`.\n",
    "\n",
    "Так же в DataLoader есть параметр `shuffle`, который используется для перемешивания данных. Сейчас наши аудио отсортированы по длине, т.е. длины аудио внутри одного батча максимально близки друг другу - и это самый эффективный способ формировать батчи в распознавании речи. Если включить shuffle=True, то короткие аудио могут попасть в один батч с длинными, средний размер батча увеличится, это будет менее эффективно. Поэтому необходимо **перемешивать сформированные батчи**, а не элементы датасета.\n",
    "    \n",
    "Реализовать перемешивание батчей в PyTorch - это не самая простая задача, поэтому советую сейчас пропустить этот шаг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = # YOUR CODE\n",
    "val_dataloader = # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что у нас хранится в даталоадере. Обратите внимание, что из-за `batch_size=90` выведется достаточно много значений. Можно уменьшить `batch_size`, чтобы посмотреть на выход, но обучать сеть лучше с `batch_size=90`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element number: 0\n",
      "{'audio_lens': tensor([8360, 8640, 8987, 9196, 9196]),\n",
      " 'audios': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.3758e-09, -2.2855e-09, -1.3640e-09,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.2861e-03,\n",
      "          2.3552e-03,  9.7071e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.5030e-03,\n",
      "         -1.5566e-03, -8.6835e-04]]),\n",
      " 'text_lens': tensor([4, 4, 3, 5, 3]),\n",
      " 'texts': ['один', 'ноль', 'два', 'шесть', 'нет'],\n",
      " 'tokens': tensor([[15,  4,  9, 14,  0],\n",
      "        [14, 15, 12, 27,  0],\n",
      "        [ 4,  2,  0,  0,  0],\n",
      "        [25,  5, 18, 19, 27],\n",
      "        [14,  5, 19,  0,  0]])}\n",
      "\n",
      "element number: 1\n",
      "{'audio_lens': tensor([ 9405,  9405,  9600,  9823, 10032]),\n",
      " 'audios': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.2060e-10, -1.2447e-09, -2.0906e-09,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.6213e-05,\n",
      "          2.4258e-04, -8.4436e-05]]),\n",
      " 'text_lens': tensor([4, 3, 3, 5, 4]),\n",
      " 'texts': ['пять', 'три', 'два', 'шесть', 'ноль'],\n",
      " 'tokens': tensor([[16, 32, 19, 27,  0],\n",
      "        [19, 17,  9,  0,  0],\n",
      "        [ 4,  2,  0,  0,  0],\n",
      "        [25,  5, 18, 19, 27],\n",
      "        [14, 15, 12, 27,  0]])}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(train_dataloader):\n",
    "    print(f'element number: {i}')\n",
    "    pprint(element)\n",
    "    print()\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Акустические фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы обсуждали на лекциях, есть разные способы построить аудио фичи. Мы будем использовать log mel spectrogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_log_mel_spectrogram(audio, sequence_lengths,\n",
    "                            sample_rate=8000,\n",
    "                            window_size=0.02,\n",
    "                            window_step=0.01,\n",
    "                            f_min=20,\n",
    "                            f_max=3800,\n",
    "                            n_mels=64,\n",
    "                            window_fn=torch.hamming_window,\n",
    "                            power=1.0,\n",
    "                            eps=1e-6\n",
    "                            ):\n",
    "    \"\"\" Compute log-mel spectrogram.\n",
    "        Input shape:\n",
    "            audio: 3D tensor with shape (batch_size, num_timesteps)\n",
    "            sequence_lengths: 1D tensor with shape (batch_size)\n",
    "        Returns:\n",
    "            4D tensor with shape (batch_size, n_mels, new_num_timesteps)\n",
    "            1D tensor with shape (batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    win_length = int(window_size * sample_rate)\n",
    "    hop_length = int(window_step * sample_rate)\n",
    "    \n",
    "    ### write your code here ###\n",
    "    \n",
    "    return log_mel_spectrogram, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_compute_log_mel_spectrogram.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как выглядит log-mel спектрограмма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "audio, audio_len = open_audio('test_files/test_audio.opus', 8000)\n",
    "spectrogram, new_len = compute_log_mel_spectrogram(audio, audio_len)\n",
    "plt.pcolormesh(spectrogram)\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('mels')\n",
    "plt.show()\n",
    "Audio(data=audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы подготовили все данные, теперь можно заняться реализацией нейронной сети. Будем реализовывать архитектуру [Deepspeech 2](https://arxiv.org/pdf/1512.02595.pdf) в немного упрощенном виде.\n",
    "\n",
    "\n",
    "<img src=\"images/cat_reproduction.jpg\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так будет выглядеть архитектура сети:\n",
    "\n",
    "\n",
    "<img src=\"images/deepspeech.jpg\" width=\"200\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_mel_bins, hidden_size, num_layers, num_tokens):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        conv1_params = AttrDict(\n",
    "            {\n",
    "                \"num_filters\": 32,\n",
    "                \"kernel_size\": [21, 11],\n",
    "                \"stride\": [1, 1] \n",
    "            })                             \n",
    "        conv2_params = AttrDict(\n",
    "            {\n",
    "                \"num_filters\": 64,\n",
    "                \"kernel_size\": [11, 11],\n",
    "                \"stride\": [1, 3]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.conv1_params = conv1_params\n",
    "        self.conv2_params = conv2_params\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # CONV 1\n",
    "            # BATCH NORM 1\n",
    "            # RELU\n",
    "            # CONV 2\n",
    "            # BATCH NORM 2\n",
    "            # RELU\n",
    "            \n",
    "            ### write your code here ###\n",
    "        )\n",
    "\n",
    "        \n",
    "        rnn_input_size = # YOUR CODE\n",
    "\n",
    "        # 4 слоя бидир lstm\n",
    "        self.lstm = # YOUR CODE\n",
    "        \n",
    "        self.output_layer = # YOUR CODE\n",
    "\n",
    "    def forward(self, inputs, seq_lens, state=None):\n",
    "        \"\"\"\n",
    "            Input shape:\n",
    "                audio: 3D tensor with shape (batch_size, num_mel_bins, num_timesteps)\n",
    "                sequence_lengths: 1D tensor with shape (batch_size)\n",
    "            Returns:\n",
    "                3D tensor with shape (new_num_timesteps, batch_size, alphabet_len)\n",
    "                1D tensor with shape (batch_size)\n",
    "            \"\"\"\n",
    "        \n",
    "        outputs = inputs.unsqueeze(1) # conv2d input should be four-dimensional\n",
    "        \n",
    "        ### write your code here ###\n",
    "\n",
    "\n",
    "        seq_lens = self.get_new_seq_lens(seq_lens,\n",
    "                                         self.conv1_params.kernel_size[1], self.conv1_params.stride[1],\n",
    "                                         self.conv2_params.kernel_size[1], self.conv2_params.stride[1])\n",
    "        return F.log_softmax(outputs, dim=-1), seq_lens\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose_and_reshape(inputs):\n",
    "    \n",
    "        \"\"\" This function will be very useful for converting the output of a convolutional layer \n",
    "            to the input of a lstm layer\n",
    "            \n",
    "            Input shape:\n",
    "                inputs: 4D tensor with shape (batch_size, num_filters, num_features, num_timesteps)\n",
    "            Returns:\n",
    "                3D tensor with shape (batch_size, num_timesteps, new_num_features)\n",
    "            \"\"\"\n",
    "            \n",
    "        sizes = inputs.size()\n",
    "                \n",
    "        # reshape # YOUR CODE\n",
    "        # (batch_size, num_filters * num_features, num_timesteps)\n",
    "        \n",
    "        # transpose # YOUR CODE\n",
    "        # (batch_size, num_timesteps, new_num_features)\n",
    "        \n",
    "        return outputs\n",
    "           \n",
    "    @staticmethod\n",
    "    def get_new_seq_lens(seq_lens, conv1_kernel_size, conv1_stride, conv2_kernel_size, conv2_stride):\n",
    "    \n",
    "        \"\"\" Compute sequence_lengths after convolutions\n",
    "            \"\"\"\n",
    "            \n",
    "        ### write your code here ###\n",
    "        return seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_mel_bins = 64\n",
    "hidden_size= 512\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_mel_bins=num_mel_bins,\n",
    "              hidden_size=hidden_size,\n",
    "              num_layers=num_layers,\n",
    "              num_tokens=num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest tests/test_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель из чекпоинта, чтобы она обучилась быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_from_ckpt(model, ckpt_path):\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_from_ckpt(model, '/data/ckpt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест, чтобы проверить, что код модели корректно написан, модель правильно восстановилась из чекпоинта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, audio_len = open_audio('test_files/test_audio.mp3', sample_rate)\n",
    "output = model(*compute_log_mel_spectrogram(torch.unsqueeze(audio, 0), torch.unsqueeze(torch.tensor([audio_len]), 0)))\n",
    "assert torch.isclose(output[0][0][0][0], torch.tensor(-3.53916406))\n",
    "assert torch.isclose(output[0][15][0][30], torch.tensor(-3.605963468))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отправим модель на гпу.\n",
    "\n",
    "<img src=\"images/cuda_is_important.jpg\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем модельку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте напишем несколько вспомогательных функций, которые будут нам нужны для обучения модели.\n",
    "\n",
    "Для начала займемся метриками. Основная метрика - wer (word error rate).\n",
    "\n",
    "Тут поможет библиотека `editdistance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_wer(predicted_text, gt_text):\n",
    "    \"\"\" Compute wer.\n",
    "        Inputs:\n",
    "            predicted_text: str\n",
    "            gt_text: str\n",
    "        Returns:\n",
    "            wer: int\n",
    "    \"\"\"\n",
    "    ### write your code here ###\n",
    "    \n",
    "    return wer\n",
    "\n",
    "\n",
    "def calc_wer_for_batch(list_of_predicted_text, list_of_gt_text):\n",
    "    \"\"\" Compute mean wer for batch.\n",
    "            Inputs:\n",
    "                list_of_predicted_text: list\n",
    "                list_of_gt_text: list\n",
    "            Returns:int\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    ### write your code here ###\n",
    "    \n",
    "    return mean_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_compute_wer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте реализуем greedy decoding. \n",
    "\n",
    "Сначала научимся получать greedy trn из выравниваний. Можно использовать `itertools`.\n",
    "\n",
    "Не забудьте выкинуть лишние пробелы в начале и конце полученного текста!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode(alignment):\n",
    "    \"\"\" Get text from alignment.\n",
    "        Inputs:\n",
    "            alignment: str\n",
    "        Returns:\n",
    "            text: srt\n",
    "    \"\"\"\n",
    "    ### write your code here ###\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_decode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим greedy text из выхода акустической модели (logprobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(logprobs, logprobs_lens, vocab):\n",
    "    \"\"\" Compute greedy text from loglikes.\n",
    "            Input shape:\n",
    "                logprobs: 3D tensor with shape (num_timesteps, batch_size, alphabet_len)\n",
    "                logprobs_lens: 1D tensor with shape (batch_size)\n",
    "            Returns:\n",
    "                list of texts with len (batch_size)\n",
    "        \"\"\"\n",
    "    ### write your code here ###\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_get_prediction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой функции надо из сырых данных, извлеченных из датасета, получить спектрограмму, прогнать через модель, посчитать средний лосс и wer для батча, list с текстами предсказанных гипотез для батча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_results(model, audio, audio_lens, tokens, text, text_lens, vocab, loss_fn):\n",
    "    \"\"\" get mean loss, mean wer and prediction list for batch\n",
    "        Returns:\n",
    "            loss: int\n",
    "            wer: int\n",
    "            prediction: list of str\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    ### write your code here ###\n",
    "    \n",
    "    return loss, wer, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства будем логировать метрики в [tensorboard](https://pytorch.org/docs/stable/tensorboard.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TensorboardLogger:\n",
    "    def __init__(self, tensorboard_path):\n",
    "        self.writer = SummaryWriter(tensorboard_path)\n",
    "\n",
    "    def log(self, step, loss, wer, mode):\n",
    "        assert mode in ('train', 'val')\n",
    "        \n",
    "        ### write your code here ###\n",
    "        \n",
    "        # add loss to tb \n",
    "        # add wer to tb \n",
    "\n",
    "    def log_text(self, step, pred_texts, gt_texts, mode):\n",
    "        \n",
    "        ### write your code here ###\n",
    "        \n",
    "        for pred_text in pred_texts:\n",
    "            # add pred text to tb \n",
    "            \n",
    "        for gt_text in gt_texts:\n",
    "            # add gt text to tb \n",
    "      \n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем собрать функции в train loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, loss_fn, num_epochs, train_dataloader, val_dataloader, log_every_n_batch, model_dir,\n",
    "             vocab):\n",
    "\n",
    "    logger = TensorboardLogger(model_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start_time = time()\n",
    "        train_loss, train_wer = 0, 0\n",
    "        model.train(True)\n",
    "\n",
    "        for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
    "            loss, wer, prediction = get_model_results(model, batch[\"audios\"], batch[\"audio_lens\"],\n",
    "                                                      batch[\"tokens\"], batch[\"texts\"],\n",
    "                                                      batch[\"text_lens\"], vocab, loss_fn)\n",
    "\n",
    "\n",
    "\n",
    "            # optimizer step\n",
    "            ### write your code here ###\n",
    "             \n",
    "             \n",
    "            \n",
    "\n",
    "            train_loss += loss.cpu().data.numpy()\n",
    "            train_wer += wer\n",
    "\n",
    "            step = len(train_dataloader) * epoch + iteration\n",
    "            if step % log_every_n_batch == 0:\n",
    "                logger.log(step, loss, wer, 'train')\n",
    "                logger.log_text(step, prediction, batch[\"texts\"], \"train\")\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_wer /= len(train_dataloader)\n",
    "\n",
    "        val_loss, val_wer = 0, 0\n",
    "        model.train(False)\n",
    "\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            loss_val, wer, prediction = get_model_results(model, batch[\"audios\"], batch[\"audio_lens\"],\n",
    "                                                          batch[\"tokens\"], batch[\"texts\"],\n",
    "                                                          batch[\"text_lens\"], vocab, loss_fn)\n",
    "            val_loss += loss_val.cpu().data.numpy()\n",
    "            val_wer += wer\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_wer /= len(val_dataloader)\n",
    "\n",
    "        logger.log(step, val_loss, val_wer, 'val')\n",
    "        logger.log_text(step, prediction, batch[\"texts\"], \"val\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, os.path.join(model_dir, f'epoch_{epoch}.pt'))\n",
    "\n",
    "        print(f'\\nEpoch {epoch + 1} of {num_epochs} took {time() - start_time}s, ' + \\\n",
    "              f'train loss: {train_loss}, val loss: {val_loss}, train wer: {train_wer}, val wer: {val_wer}')\n",
    "\n",
    "    logger.close()\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 7\n",
    "model_dir = 'models/1'\n",
    "log_every_n_batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CTCLoss(blank=blank_index, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про ctc loss очень хорошо написано [тут](https://distill.pub/2017/ctc/). А [это](https://www.cs.toronto.edu/~graves/icml_2006.pdf) исходная статья."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если не используете перемешивание батчей (шафл), то при подборе batch size обратите внимание, что данные отсортированы (обучение будет замедляться с увеличением длины аудио)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training(model, opt, loss_fn, num_epochs, train_dataloader, val_dataloader, log_every_n_batch,\n",
    "             model_dir, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/training.jpeg\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "После того, как напишете весь код и запустите обучение вашей первой модели, примерно после 5-6 эпох качество модели достигнет 30-35% WER.\n",
    "\n",
    "# Как можно улучшить полученные результаты\n",
    "\n",
    "<img src=\"images/pronunciation_or_not.jpg\" width=\"400\" height=\"400\">\n",
    "\n",
    "Далее описаны несколько способов, которые могут помочь улучшить качество. Потенциальный прирост обозначим ★, чем больше звездочек, тем более хорошее улучшение качества можно ожидать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search ★★\n",
    "\n",
    "На лекции обсуждали, что beam search помогает достичь более хорошего качества, чем greedy декодирование.\n",
    "\n",
    "Изучить алгоритм можно [тут](https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306) и [тут](https://drive.google.com/viewerng/viewer?url=https://arxiv.org/pdf/1408.2873.pdf)\n",
    "\n",
    "Код для beam search посмотреть [тут](https://github.com/PaddlePaddle/DeepSpeech/blob/master/decoders/decoders_deprecated.py). (Отредактировать при необходимости )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Внешняя языковая модель ★★★\n",
    "\n",
    "\n",
    "Внешняя языковая модель позволяет улучшить качество, т.к. убирает условную независимость соседних символов, которая свойственна ctc лоссу.\n",
    "\n",
    "Этот пункт состоит из двух этапов: сначала надо обучить языковую модель, затем встроить ее в beam search.\n",
    "\n",
    "#### Обучение\n",
    "\n",
    "Для обучения n-gram языковой модели можно использовать фреймворк kenlm.\n",
    "Документация: \n",
    "* https://kheafield.com/code/kenlm/\n",
    "* https://github.com/kpu/kenlm\n",
    "\n",
    "Модель сначала строится в формате arpa, затем ее лучше перевести в формат trie. Вызывать полученную модель можно через питон [ссылка](https://github.com/kpu/kenlm#python-module)\n",
    "\n",
    "\n",
    "#### Данные\n",
    "\n",
    "Для обучения модели конечно нужны данные :) Тут есть варианты:\n",
    "\n",
    "1. Можно обучить маленькую языковую модель на текстах из акустических обучающих данных (из трейна!).\n",
    "\n",
    "минусы: этих данных мало\n",
    "\n",
    "плюсы: домен остается таким же\n",
    "\n",
    "2. Можно взять внешние данные, например, [отсюда](http://data.statmt.org/cc-100/). (46G).\n",
    "\n",
    "минусы: тексты из другого (произвольного) домена\n",
    "\n",
    "плюсы: данных много\n",
    "\n",
    "При необходимости, данные надо предобработать - привести к нижнему регистру, разделить на предложения. Убрать предложения, которые содержат символы не из русского алфавита \n",
    "\n",
    "\n",
    "#### Внедрение в beam search\n",
    "\n",
    "В разделе про beam search есть ссылки на алгоритм.\n",
    "\n",
    "Можно использовать аргумент `ext_scoring_func` [тут](https://github.com/PaddlePaddle/DeepSpeech/blob/master/decoders/decoders_deprecated.py#L47).\n",
    "\n",
    "Пример скорера можно найти [тут](https://github.com/PaddlePaddle/DeepSpeech/blob/master/decoders/scorer_deprecated.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментации ★\n",
    "\n",
    "\n",
    "Позволяют искусственно увеличь размер обучающей выборки, сделать его более разнообразным.\n",
    "\n",
    "### Аугментации аудио \n",
    "\n",
    "Применяются к аудиосигналу. Аугментации обычно реализуются через [sox](https://ru.wikipedia.org/wiki/SoX_(%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0)). [Тут](http://sox.sourceforge.net/sox.html#EFFECTS) можно посмотротреть полный список sox эффектов с описанием. \n",
    "\n",
    "Полный список sox эффектов, доступных в torchaudio, можно посмотреть [тут](https://github.com/pytorch/audio/issues/260).\n",
    "\n",
    "Эффекты можно комбинировать.\n",
    "\n",
    "Внимание!\n",
    "\n",
    "* Аугментации надо применять очень аккуратно (!) - слишком сильные аугментации только ухудшат качество. Лучше применять аугментации с некоторой вероятностью.\n",
    "* Применять **только на обучающую выборку, не на валидацию!**\n",
    "* Некоторые аугментации меняют sample rate и длину аудио.\n",
    "* Можно применять не открывая предварительно аудиофайл [ссылка](https://pytorch.org/audio/stable/sox_effects.html#applying-effects-on-file).\n",
    "\n",
    "\n",
    "Примеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio, sample_rate, effects):\n",
    "    \n",
    "    effects = [effects, ['rate', '8000']]\n",
    "\n",
    "    augmented_audio, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
    "        torch.unsqueeze(audio, 0),\n",
    "        sample_rate=sample_rate,\n",
    "        effects=effects,\n",
    "        channels_first=True)\n",
    "    \n",
    "    return augmented_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000\n",
    "audio, audio_len = open_audio('test_files/test_audio.mp3', sample_rate)\n",
    "Audio(data=audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['treble', '20'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['bass', '20'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['pitch', '400'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['speed', '1.5'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['tempo', '1.5'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментации спектрограммы\n",
    "\n",
    "[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/pdf/1904.08779.pdf)\n",
    "\n",
    "Методов [Frequency masking](https://pytorch.org/audio/stable/transforms.html#frequencymasking) и [Time masking](https://pytorch.org/audio/stable/transforms.html#timemasking) должно быть достаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что еще можно попробовать: \n",
    "\n",
    "1. Поэкспериментировать с learning rate, оптимайзером (например, взять SGD). Можно добавить [lr decay](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ExponentialLR) ★\n",
    "2. Поэкспериментировать с нейронной сетью и восттановлением из чекпоинта. Например, зафиксировать предобученные слои и дообучить остальные, потом с маленьким learning rate дообучить всю модель. ★\n",
    "3. Добавить новые слои в нейронную сеть. ★★\n",
    "4. Использовать больше данных. ★★★\n",
    "<img src=\"images/more_data.jpg\" width=\"400\" height=\"400\">\n",
    "\n",
    "В этом случае для ускорения можно запустить распределенное обучение на нескольких gpu с помощью [horovod](https://github.com/horovod/horovod). Данные можно взять тут:\n",
    "\n",
    "* [open_stt](https://github.com/snakers4/open_stt) (до 2.5 TB данных!)\n",
    "* [Russian LibriSpeech](https://openslr.org/96/) (9 GB данных)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Не использовать чекпоинт и обучить свою сеть :) ★ (потребуется больше данных!)\n",
    "\n",
    "6\\. Shallow fusion - можно обучить дополнительную языковую модель и использовать в качестве рескорера \n",
    "[ссылка](https://arxiv.org/pdf/1503.03535.pdf). ★\n",
    "\n",
    "7\\. Реализовать перемешивание батчей. ★\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
