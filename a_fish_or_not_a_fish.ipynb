{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/polina/.local/lib/python2.7/site-packages (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": [
     "torch"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eb42ca6e4af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределите в команде гпу, задайте свой номер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuda_device = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем полезную либу `attrdict`. Чем она хороша: позволяет обращаться к элементам словаря, как к его атрибутам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from attrdict import AttrDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с аудио и текстом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В распознавании речи нейронная сеть обучается на парах аудио+текст.\n",
    "\n",
    "Давайте научимся открывать аудиофайлы и подготавливать их для работы с нейронной сетью.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "На лекциях мы обсуждали, что аудио может быть записано с разной частотой дискретизации (sample rate), но для обучения нейронной сети обычно все аудио приводят к одной частоте дискретизации. (В этом проекте мы будем использовать sample rate 8000).\n",
    "\n",
    "Так же аудио может быть одноканальным или многоканальным. Проблема с многоканальным аудио может быть в том, что речь в двух каналах может звучать одновременно, такие аудио надо разделять на 2 дорожки. В нашем же случае данные подобраны таким образом, что такой ситуации не возникет, поэтому несколько каналов можно просто усреднить.\n",
    "\n",
    "\n",
    "Будем использовать библиотеку `torchaudio` [docs](https://pytorch.org/audio/).\n",
    "\n",
    "Реализуйте функцию `open_audio`, которая открывает аудио (искать [тут](https://pytorch.org/audio/stable/torchaudio.html)), усредняет аудио по всем каналам (это можно сделать обычным усреднением) и приводит к необходимой частоте дискретизации (искать [тут](https://pytorch.org/audio/stable/transforms.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "torchaudio"
    ]
   },
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"test_files/test_audio.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "open_audio"
    ]
   },
   "outputs": [],
   "source": [
    "def open_audio(audio_path, desired_sample_rate):\n",
    "    \n",
    "    \"\"\" Open and resample audio, average across channels\n",
    "        Inputs:\n",
    "            audio_path: str, path to audio\n",
    "            desired_sample_rate: int, the sampling rate to which we would like to convert the audio\n",
    "        Returns:\n",
    "            audio: 1D tensor with shape (num_timesteps)\n",
    "            audio_len: int, len of audio\n",
    "    \"\"\"\n",
    "    \n",
    "    audio_tensor = torchaudio.load(audio_path, channels_first=True)\n",
    "    orig_freq = audio_tensor[1]\n",
    "    audio = torchaudio.transforms.Resample(orig_freq, desired_sample_rate).forward(audio_tensor[0])\n",
    "    audio = torch.mean(audio, axis=0)\n",
    "    audio_len = audio.shape[0]\n",
    "    \n",
    "    ### write your code here ###\n",
    "    \n",
    "    return audio, audio_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите тесты, чтобы проверить себя.\n",
    "<img src=\"images/tests_are_all_we_need.png\" width=\"400\" height=\"600\">\n",
    "\n",
    "\n",
    "**Перед каждым запуском тестов не забывайте сохранять ноутбук.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_open_audio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте откроем аудио, и послушаем, что у нас получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_rate=8000\n",
    "audio, audio_len = open_audio('test_files/test_audio.mp3', sample_rate)\n",
    "Audio(data=audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же послушать аудио можно через путь к аудио файлу.\n",
    "\n",
    "Можно заметить, что звучание немного поменялось. Это произошло из-за того, что мы поменяли оригинальный sample rate 48000Hz на 8000Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio('test_files/test_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech2text  —  это не только speech, но и text, поэтому теперь давайте поговорим о предобработке текста. \n",
    "\n",
    "Первым шагом необходимо привести текст к нижнему регистру и удалить пунктуацию. Если на этом этапе в тексте содержатся символы кроме русского алфавита и пробела (например, цифры), то такие примеры лучше вообще убрать из обучающей выборки (если просто убрать символ из строки, то может нарушиться соответствие аудио-текст).\n",
    "\n",
    "В размеченных данных для обучения и валидации уже произедена очистка и удаление ненужных символов, поэтому нам надо только извлечь токены из текста.\n",
    "\n",
    "В качестве токенов для обучения нейронной сети будем использовать буквы русского алфавита и пробел. Так же, как было сказано в лекциях, нам потребуется специальный символ `<blank>` для построения выравниваний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "alphabet"
    ]
   },
   "outputs": [],
   "source": [
    "alphabet = ['а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к',\n",
    "            'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц',\n",
    "            'ч', 'ш', 'щ', 'ь', 'ы', 'ъ', 'э', 'ю', 'я',\n",
    "            ' ', '<blank>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но, естественно, в таком виде передавать буквы в нейронную сеть мы не можем — их нужно закодировать в числа. Для этого будем использовать уже готовую функцию Vocab.\n",
    "\n",
    "Обратите внимание, что Vocab добавляет дополнительный токен `<unk>`, ответственный за все символы, которых нет в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "vocab"
    ]
   },
   "outputs": [],
   "source": [
    "from vocabulary import Vocab\n",
    "vocab = Vocab(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попрактикуйтесь с Vocab. Для того, чтобы понять все возможности Vocab можно заглянуть в код vocabulary.py и почитать докстринги.\n",
    "\n",
    "* переведите \"привет\" в индексы\n",
    "* переведите [11, 0, 11, 33, 4, 5, 12, 0] в текст\n",
    "* что будет если перевести в индексы слово \"hi\"?\n",
    "* а [44, 5] в текст?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.lookup_indices('привет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.lookup_tokens([11, 0, 11, 33, 4, 5, 12, 0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.lookup_indices('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.lookup_tokens([44, 5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие еще у Vocab возможности?\n",
    "\n",
    "Если в процессе исследования вы изменили текущий vocab (путем добавления нового токена, например), не забудьте вернуть vocab к начальному состоянию  `vocab = Vocab(alphabet)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab.append_token('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте извлечем из Vocab и сохраним в переменные два важных значения - длину алфавита и значения индекса `<blank>`.\n",
    "    \n",
    "Эти значения нам еще много раз пригодятся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "get_num_tokens",
     "get_blank_index"
    ]
   },
   "outputs": [],
   "source": [
    "def get_num_tokens(vocab):\n",
    "    ### write your code here ###\n",
    "    num_tokens= vocab.__len__()\n",
    "    return num_tokens\n",
    "\n",
    "def get_blank_index(vocab):\n",
    "    vocab.append_token('<blank>')\n",
    "    blank_index = vocab.__getitem__('<blank>')\n",
    "    return blank_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_num_tokens.py\n",
    "! pytest tests/test_blank_index.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_tokens = get_num_tokens(vocab) \n",
    "blank_index = get_blank_index(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать нейронную сеть мы будем на подготовленном датасете. Для удобства мы сгруппировали обучающие и валидационные данные в текстовые файлы, где в каждой строке через запятую указан путь к файлу, соответствующий текст, длина аудио (в секундах).\n",
    "\n",
    "\n",
    "Давайте реализуем функцию AudioDataset, которая подготавливает текст и аудио для каждого элемента датастета.\n",
    "\n",
    "\n",
    "На этих данных мы будем обучать модель с помощью ctc лосса, а мы уже знаем, что для обучения с ctc аудио лучше отсортировать по длине - так модели будет проще обучиться.\n",
    "\n",
    "Изучить документацию по Dataset и DataLoader (пригодится далее) можно найти [тут](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path,\n",
    "                 vocab,\n",
    "                 sample_rate=8000,\n",
    "                 ):\n",
    "        self.vocab = vocab\n",
    "        self.sample_rate = sample_rate\n",
    "        data = pd.read_csv(dataset_path, header=None, names=['audio_path', 'text', 'duration'])\n",
    "        data['duration'] = data['duration'].astype(float)\n",
    "        self.data = data.sort_values(by='duration')\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.data.audio_path[idx]\n",
    "        text = self.data.text[idx]\n",
    "        text_len = len(text)\n",
    "        \n",
    "        audio, audio_len = open_audio(audio_path, self.sample_rate)\n",
    "        tokens = torch.tensor(self.vocab.lookup_indices(text))\n",
    "        \n",
    "        ### write your code here ###\n",
    "        \n",
    "        return {\"audio\":  audio,  # torch tensor, (num_timesteps)\n",
    "                \"audio_len\": audio_len, # int\n",
    "                \"text\": text, # str\n",
    "                \"text_len\": text_len, # int\n",
    "                'tokens': tokens, # torch tensor, (text_len)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(\"test_files/test_dataset.txt\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как будут выглядеть элементы полученного датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = '/home/e.chuykova/data/train.txt'\n",
    "val_dataset = '/home/e.chuykova/data/val.txt'\n",
    "test_dataset = '/home/e.chuykova/data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(train_dataset, vocab)\n",
    "\n",
    "for i, element in enumerate(dataset):\n",
    "    print(f'element number: {i}')\n",
    "    pprint(element)\n",
    "    print()\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы эффективно обучать нейронную сеть, необходимо подавать в нее данные батчами. В этом нам поможет функция `torch.utils.data.DataLoader`. \n",
    "\n",
    "Обратите внимание, что некоторые данные в датасете разной длины (например, `audio`), для формирования батча из таких данных  необходимо использовать паддинг. Для этого можно реализовать фукцию `collate_fn`. Подробнее про то, как именно использовать `collate_fn` можно почитатать в доках к `torch.utils.data.DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\" \n",
    "        Inputs:\n",
    "            batch: list of elements with length=batch_size\n",
    "        Returns:\n",
    "            dict\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    ### write your code here ###\n",
    "    audios = []\n",
    "    audio_lens = []\n",
    "    texts = []\n",
    "    text_lens = []\n",
    "    tokens = []\n",
    "    \n",
    "    for el in batch:\n",
    "        audios.append(el['audio'])\n",
    "        audio_lens.append(el['audio_len'])\n",
    "        texts.append(el['text'])\n",
    "        text_lens.append(el['text_len'])\n",
    "        tokens.append(el['tokens'])\n",
    "        \n",
    "    audios = pad_sequence(audios, batch_first=True).view(len(batch), -1)\n",
    "    audio_lens = torch.tensor(audio_lens)\n",
    "    text_lens = torch.tensor(text_lens)\n",
    "    tokens = pad_sequence(tokens, batch_first=True).view(len(batch), -1)\n",
    "\n",
    "    return {'audios': audios, # torch tensor, (batch_size, max_num_timesteps)\n",
    "            'audio_lens': audio_lens, # torch tensor, (batch_size)\n",
    "            'texts': texts,  # list, len=(batch_size)\n",
    "            \"text_lens\": text_lens, # torch tensor, (batch_size)\n",
    "            'tokens': tokens,  # torch tensor, (batch_size, max_text_len)\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_collate_fn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 90\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании DataLoader не забудьте использовать параметры `batch_size`, `num_workers`.\n",
    "\n",
    "Так же в DataLoader есть параметр `shuffle`, который используется для перемешивания данных. Сейчас наши аудио отсортированы по длине, т.е. длины аудио внутри одного батча максимально близки друг другу - и это самый эффективный способ формировать батчи в распознавании речи. Если включить shuffle=True, то короткие аудио могут попасть в один батч с длинными, средний размер батча увеличится, это будет менее эффективно. Поэтому необходимо **перемешивать сформированные батчи**, а не элементы датасета.\n",
    "    \n",
    "Реализовать перемешивание батчей в PyTorch - это не самая простая задача, поэтому советую сейчас пропустить этот шаг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_audiodataset = AudioDataset(train_dataset, vocab)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_audiodataset, batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_audiodataset = AudioDataset(val_dataset, vocab)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_audiodataset, batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что у нас хранится в даталоадере. Обратите внимание, что из-за `batch_size=90` выведется достаточно много значений. Можно уменьшить `batch_size`, чтобы посмотреть на выход, но обучать сеть лучше с `batch_size=90`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, element in enumerate(train_dataloader):\n",
    "    print(f'element number: {i}')\n",
    "    pprint(element)\n",
    "    print()\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Акустические фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы обсуждали на лекциях, есть разные способы построить аудио фичи. Мы будем использовать log mel spectrogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_log_mel_spectrogram(audio, sequence_lengths,\n",
    "                            sample_rate=8000,\n",
    "                            window_size=0.02,\n",
    "                            window_step=0.01,\n",
    "                            f_min=20,\n",
    "                            f_max=3800,\n",
    "                            n_mels=64,\n",
    "                            window_fn=torch.hamming_window,\n",
    "                            power=1.0,\n",
    "                            eps=1e-6\n",
    "                            ):\n",
    "    \"\"\" Compute log-mel spectrogram.\n",
    "        Input shape:\n",
    "            audio: 3D tensor with shape (batch_size, num_timesteps)\n",
    "            sequence_lengths: 1D tensor with shape (batch_size)\n",
    "        Returns:\n",
    "            4D tensor with shape (batch_size, n_mels, new_num_timesteps)\n",
    "            1D tensor with shape (batch_size)\n",
    "    \"\"\"\n",
    "    win_length = int(window_size * sample_rate)\n",
    "    hop_length = int(window_step * sample_rate)\n",
    "    n_fft = win_length\n",
    "    \n",
    "    log_mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=win_length, \n",
    "    win_length=win_length, hop_length=hop_length, f_min=f_min, f_max=f_max,n_mels=n_mels,\n",
    "                                window_fn=window_fn,power=power).forward(audio)\n",
    "    log_mel_spectrogram = torch.log(log_mel_spectrogram + eps)\n",
    "    \n",
    "    sequence_lengths = ((sequence_lengths + 2 * hop_length - win_length) // hop_length + 1).long()\n",
    "    \n",
    "    return log_mel_spectrogram, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = np.load('test_files/test_open_audio.npy')\n",
    "compute_log_mel_spectrogram(torch.tensor(audio), torch.tensor(len(audio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_compute_log_mel_spectrogram.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как выглядит log-mel спектрограмма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# audio, audio_len = open_audio('test_files/test_audio.opus', 8000)\n",
    "# spectrogram, new_len = compute_log_mel_spectrogram(audio, audio_len)\n",
    "# plt.pcolormesh(spectrogram)\n",
    "# plt.xlabel('T')\n",
    "# plt.ylabel('mels')\n",
    "# plt.show()\n",
    "# Audio(data=audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы подготовили все данные, теперь можно заняться реализацией нейронной сети. Будем реализовывать архитектуру [Deepspeech 2](https://arxiv.org/pdf/1512.02595.pdf) в немного упрощенном виде.\n",
    "\n",
    "\n",
    "<img src=\"images/cat_reproduction.jpg\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так будет выглядеть архитектура сети:\n",
    "\n",
    "\n",
    "<img src=\"images/deepspeech.jpg\" width=\"200\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_mel_bins, hidden_size, num_layers, num_tokens):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        conv1_params = AttrDict(\n",
    "            {\n",
    "                \"num_filters\": 32,\n",
    "                \"kernel_size\": [21, 11],\n",
    "                \"stride\": [1, 1] \n",
    "            })                             \n",
    "        conv2_params = AttrDict(\n",
    "            {\n",
    "                \"num_filters\": 64,\n",
    "                \"kernel_size\": [11, 11],\n",
    "                \"stride\": [1, 3]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.conv1_params = conv1_params\n",
    "        self.conv2_params = conv2_params\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=conv1_params.num_filters,\n",
    "            kernel_size=conv1_params.kernel_size,\n",
    "            stride=conv1_params.stride,\n",
    "            bias=False),\n",
    "            nn.BatchNorm2d(conv1_params.num_filters,\n",
    "                          momentum=0.9),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=conv1_params.num_filters,\n",
    "            out_channels=conv2_params.num_filters,\n",
    "            kernel_size=conv2_params.kernel_size,\n",
    "            stride=conv2_params.stride,\n",
    "            bias=False),\n",
    "            nn.BatchNorm2d(conv2_params.num_filters,\n",
    "                          momentum=0.9),\n",
    "            nn.ReLU()\n",
    "            # CONV 1\n",
    "            # BATCH NORM 1\n",
    "            # RELU\n",
    "            # CONV 2\n",
    "            # BATCH NORM 2\n",
    "            # RELU\n",
    "        )\n",
    "\n",
    "        \n",
    "        rnn_input_size = (num_mel_bins - conv1_params.kernel_size[0]) // conv1_params.stride[0] + 1\n",
    "        rnn_input_size = (rnn_input_size - conv2_params.kernel_size[0]) // conv2_params.stride[0] + 1\n",
    "        rnn_input_size *= conv2_params.num_filters\n",
    "\n",
    "        # 4 слоя бидир lstm\n",
    "        self.lstm = nn.LSTM(input_size=rnn_input_size,\n",
    "               hidden_size=hidden_size,\n",
    "               num_layers=num_layers,\n",
    "               bidirectional=True,\n",
    "               batch_first=True) \n",
    "        \n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features=hidden_size * 2,\n",
    "                                      out_features=num_tokens) # YOUR CODE\n",
    "\n",
    "    def forward(self, inputs, seq_lens, state=None):\n",
    "        \"\"\"\n",
    "            Input shape:\n",
    "                audio: 3D tensor with shape (batch_size, num_mel_bins, num_timesteps)\n",
    "                sequence_lengths: 1D tensor with shape (batch_size)\n",
    "            Returns:\n",
    "                3D tensor with shape (new_num_timesteps, batch_size, alphabet_len)\n",
    "                1D tensor with shape (batch_size)\n",
    "            \"\"\"\n",
    "        \n",
    "        outputs = inputs.unsqueeze(1) # conv2d input should be four-dimensional\n",
    "        outputs = Model.transpose_and_reshape(self.conv(outputs))\n",
    "        outputs, hidden = self.lstm(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        outputs = torch.transpose(outputs, 0, 1)\n",
    "\n",
    "\n",
    "        seq_lens = self.get_new_seq_lens(seq_lens,\n",
    "                                         self.conv1_params.kernel_size[1], self.conv1_params.stride[1],\n",
    "                                         self.conv2_params.kernel_size[1], self.conv2_params.stride[1])\n",
    "        \n",
    "        return F.log_softmax(outputs, dim=-1), seq_lens\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose_and_reshape(inputs):\n",
    "    \n",
    "        \"\"\" This function will be very useful for converting the output of a convolutional layer \n",
    "            to the input of a lstm layer\n",
    "            \n",
    "            Input shape:\n",
    "                inputs: 4D tensor with shape (batch_size, num_filters, num_features, num_timesteps)\n",
    "            Returns:\n",
    "                3D tensor with shape (batch_size, num_timesteps, new_num_features)\n",
    "            \"\"\"\n",
    "            \n",
    "        sizes = inputs.size()\n",
    "                \n",
    "        # reshape # YOUR CODE\n",
    "        \n",
    "        outputs = torch.reshape(inputs, (sizes[0], sizes[1] * sizes[2], sizes[3]))\n",
    "        # (batch_size, num_filters * num_features, num_timesteps)\n",
    "        \n",
    "        # transpose # YOUR CODE\n",
    "        \n",
    "        # (batch_size, num_timesteps, new_num_features)\n",
    "        \n",
    "        outputs = torch.transpose(outputs, 2, 1)\n",
    "        \n",
    "        return outputs\n",
    "           \n",
    "    @staticmethod\n",
    "    def get_new_seq_lens(seq_lens, conv1_kernel_size, conv1_stride, conv2_kernel_size, conv2_stride):\n",
    "    \n",
    "        \"\"\" Compute sequence_lengths after convolutions\n",
    "            \"\"\"\n",
    "        seq_lens = (seq_lens - conv1_kernel_size) // conv1_stride + 1\n",
    "        seq_lens = (seq_lens - conv2_kernel_size) // conv2_stride + 1\n",
    "        return seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_mel_bins = 64\n",
    "hidden_size= 512\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_mel_bins=num_mel_bins,\n",
    "              hidden_size=hidden_size,\n",
    "              num_layers=num_layers,\n",
    "              num_tokens=num_tokens-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pytest tests/test_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель из чекпоинта, чтобы она обучилась быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_from_ckpt(model, ckpt_path):\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_from_ckpt(model, '/home/e.chuykova/data/ckpt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест, чтобы проверить, что код модели корректно написан, модель правильно восстановилась из чекпоинта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, audio_len = open_audio('test_files/test_audio.mp3', sample_rate)\n",
    "output = model(*compute_log_mel_spectrogram(torch.unsqueeze(audio, 0), torch.unsqueeze(torch.tensor([audio_len]), 0)))\n",
    "assert torch.isclose(output[0][0][0][0], torch.tensor(-3.53916406))\n",
    "assert torch.isclose(output[0][15][0][30], torch.tensor(-3.605963468))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отправим модель на гпу.\n",
    "\n",
    "<img src=\"images/cuda_is_important.jpg\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем модельку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте напишем несколько вспомогательных функций, которые будут нам нужны для обучения модели.\n",
    "\n",
    "Для начала займемся метриками. Основная метрика - wer (word error rate).\n",
    "\n",
    "Тут поможет библиотека `editdistance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_wer(predicted_text, gt_text):\n",
    "    \"\"\" Compute wer.\n",
    "        Inputs:\n",
    "            predicted_text: str\n",
    "            gt_text: str\n",
    "        Returns:\n",
    "            wer: int\n",
    "    \"\"\"\n",
    "    predicted_text = predicted_text.split()\n",
    "    gt_text = gt_text.split()\n",
    "    wer = editdistance.distance(predicted_text, gt_text)\n",
    "    if len(gt_text) == 0:\n",
    "        if wer > 0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0\n",
    "    return wer / len(gt_text)\n",
    "\n",
    "\n",
    "def calc_wer_for_batch(list_of_predicted_text, list_of_gt_text):\n",
    "    \"\"\" Compute mean wer for batch.\n",
    "            Inputs:\n",
    "                list_of_predicted_text: list\n",
    "                list_of_gt_text: list\n",
    "            Returns:int\n",
    "            \n",
    "    \"\"\"\n",
    "    mean_wer = np.mean([calc_wer(a, b) for (a, b) in zip(list_of_predicted_text, list_of_gt_text)])\n",
    "        \n",
    "    return mean_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_compute_wer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте реализуем greedy decoding. \n",
    "\n",
    "Сначала научимся получать greedy trn из выравниваний. Можно использовать `itertools`.\n",
    "\n",
    "Не забудьте выкинуть лишние пробелы в начале и конце полученного текста!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode(alignment):\n",
    "    \"\"\" Get text from alignment.\n",
    "        Inputs:\n",
    "            alignment: str\n",
    "        Returns:\n",
    "            text: srt\n",
    "    \"\"\"\n",
    "    prev = '+'\n",
    "    text = \"\"\n",
    "    for c in alignment:\n",
    "        if c ==prev:\n",
    "            continue\n",
    "        text += c\n",
    "        prev = c\n",
    "    text = text.replace(\"<blank>\", \"\")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_decode.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим greedy text из выхода акустической модели (logprobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(logprobs, logprobs_lens, vocab):\n",
    "    \"\"\" Compute greedy text from loglikes.\n",
    "            Input shape:\n",
    "                logprobs: 3D tensor with shape (num_timesteps, batch_size, alphabet_len)\n",
    "                logprobs_lens: 1D tensor with shape (batch_size)\n",
    "            Returns:\n",
    "                list of texts with len (batch_size)\n",
    "        \"\"\"\n",
    "#     logprobs = torch.transpose(logprobs, 0,1)\n",
    "\n",
    "    argmax_result = logprobs.argmax(dim=-1)\n",
    "    prediction = []\n",
    "    for i in range(argmax_result.size(1)):\n",
    "        text = \"\".join(vocab.lookup_tokens(argmax_result[:logprobs_lens.long()[i], i].tolist()))\n",
    "        text = decode(text)\n",
    "        prediction.append(text)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pytest tests/test_get_prediction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой функции надо из сырых данных, извлеченных из датасета, получить спектрограмму, прогнать через модель, посчитать средний лосс и wer для батча, list с текстами предсказанных гипотез для батча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_results(model, audio, audio_lens, tokens, text, text_lens, vocab, loss_fn):\n",
    "    \"\"\" get mean loss, mean wer and prediction list for batch\n",
    "        Returns:\n",
    "            loss: int\n",
    "            wer: int\n",
    "            prediction: list of str\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    log_mel_spec, seq_lens = compute_log_mel_spectrogram(audio, audio_lens)\n",
    "    logprobs, logprobs_lens = model.forward(log_mel_spec.cuda(cuda_device), seq_lens.cuda(cuda_device))\n",
    "    \n",
    "    prediction = get_prediction(logprobs.detach().cpu(), logprobs_lens.detach().cpu(), vocab)\n",
    "    \n",
    "    wer = calc_wer_for_batch(prediction, text)\n",
    "    \n",
    "    loss = loss_fn(logprobs, tokens.cuda(cuda_device), logprobs_lens.long(), text_lens.long().cuda(cuda_device))\n",
    "    \n",
    "    return loss, wer, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства будем логировать метрики в [tensorboard](https://pytorch.org/docs/stable/tensorboard.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TensorboardLogger:\n",
    "    def __init__(self, tensorboard_path):\n",
    "        self.writer = SummaryWriter(tensorboard_path)\n",
    "\n",
    "    def log(self, step, loss, wer, mode):\n",
    "        assert mode in ('train', 'val')\n",
    "        \n",
    "        self.writer.add_scalar('Loss/' + mode, loss, step)\n",
    "        self.writer.add_scalar('Wer', wer, step)\n",
    "        \n",
    "        # add loss to tb \n",
    "        # add wer to tb \n",
    "\n",
    "    def log_text(self, step, pred_texts, gt_texts, mode):\n",
    "        \n",
    "        ### write your code here ###\n",
    "        \n",
    "        for pred_text in pred_texts:\n",
    "            self.writer.add_text('Predicted text', pred_text, step) \n",
    "            # add pred text to tb \n",
    "            \n",
    "        for gt_text in gt_texts:\n",
    "            self.writer.add_text('Ground true text', gt_text, step) \n",
    "            # add gt text to tb \n",
    "      \n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем собрать функции в train loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, loss_fn, num_epochs, train_dataloader, val_dataloader, log_every_n_batch, model_dir,\n",
    "             vocab):\n",
    "\n",
    "    logger = TensorboardLogger(model_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start_time = time()\n",
    "        train_loss, train_wer = 0, 0\n",
    "        model.train(True)\n",
    "\n",
    "        for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
    "            loss, wer, prediction = get_model_results(model, batch[\"audios\"], batch[\"audio_lens\"],\n",
    "                                                      batch[\"tokens\"], batch[\"texts\"],\n",
    "                                                      batch[\"text_lens\"], vocab, loss_fn)\n",
    "\n",
    "\n",
    "\n",
    "            # optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            print(f\"iter {iteration}: loss {loss}, wer {wer}, prediction {prediction}\")\n",
    "             \n",
    "             \n",
    "            \n",
    "\n",
    "            train_loss += loss.cpu().data.numpy()\n",
    "            train_wer += wer\n",
    "\n",
    "            step = len(train_dataloader) * epoch + iteration\n",
    "            if step % log_every_n_batch == 0:\n",
    "                logger.log(step, loss, wer, 'train')\n",
    "                logger.log_text(step, prediction, batch[\"texts\"], \"train\")\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_wer /= len(train_dataloader)\n",
    "\n",
    "        val_loss, val_wer = 0, 0\n",
    "        model.train(False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                loss_val, wer, prediction = get_model_results(model, batch[\"audios\"], batch[\"audio_lens\"],\n",
    "                                                          batch[\"tokens\"], batch[\"texts\"],\n",
    "                                                          batch[\"text_lens\"], vocab, loss_fn)\n",
    "                val_loss += loss_val.cpu().data.numpy()\n",
    "                val_wer += wer\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_wer /= len(val_dataloader)\n",
    "\n",
    "        logger.log(step, val_loss, val_wer, 'val')\n",
    "        logger.log_text(step, prediction, batch[\"texts\"], \"val\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, os.path.join(model_dir, f'epoch_{epoch}.pt'))\n",
    "\n",
    "        print(f'\\nEpoch {epoch + 1} of {num_epochs} took {time() - start_time}s, ' + \\\n",
    "              f'train loss: {train_loss}, val loss: {val_loss}, train wer: {train_wer}, val wer: {val_wer}')\n",
    "\n",
    "    logger.close()\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 7\n",
    "model_dir = 'models/1'\n",
    "log_every_n_batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CTCLoss(blank=blank_index, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mel_bins = 64\n",
    "hidden_size= 512\n",
    "num_layers = 4\n",
    "model = Model(num_mel_bins=num_mel_bins,\n",
    "              hidden_size=hidden_size,\n",
    "              num_layers=num_layers,\n",
    "              num_tokens=num_tokens-1)\n",
    "load_from_ckpt(model, '/home/e.chuykova/data/ckpt.pt')\n",
    "model.cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tmux kill-session -t tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tmux new-session -d -s tensorboard 'export CUDA_VISIBLE_DEVICES=\"\" && tensorboard --logdir=models --port 7781'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про ctc loss очень хорошо написано [тут](https://distill.pub/2017/ctc/). А [это](https://www.cs.toronto.edu/~graves/icml_2006.pdf) исходная статья."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если не используете перемешивание батчей (шафл), то при подборе batch size обратите внимание, что данные отсортированы (обучение будет замедляться с увеличением длины аудио)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training(model, opt, loss_fn, num_epochs, train_dataloader, val_dataloader, log_every_n_batch,\n",
    "             model_dir, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/training.jpeg\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "После того, как напишете весь код и запустите обучение вашей первой модели, примерно после 5-6 эпох качество модели достигнет 30-35% WER.\n",
    "\n",
    "# Как можно улучшить полученные результаты\n",
    "\n",
    "<img src=\"images/pronunciation_or_not.jpg\" width=\"400\" height=\"400\">\n",
    "\n",
    "Далее описаны несколько способов, которые могут помочь улучшить качество. Потенциальный прирост обозначим ★, чем больше звездочек, тем более хорошее улучшение качества можно ожидать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search ★★\n",
    "\n",
    "На лекции обсуждали, что beam search помогает достичь более хорошего качества, чем greedy декодирование.\n",
    "\n",
    "Изучить алгоритм можно [тут](https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306) и [тут](https://drive.google.com/viewerng/viewer?url=https://arxiv.org/pdf/1408.2873.pdf)\n",
    "\n",
    "Код для beam search посмотреть [тут](https://github.com/PaddlePaddle/DeepSpeech/blob/master/decoders/decoders_deprecated.py). (Отредактировать при необходимости )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Внешняя языковая модель ★★★\n",
    "\n",
    "\n",
    "Внешняя языковая модель позволяет улучшить качество, т.к. убирает условную независимость соседних символов, которая свойственна ctc лоссу.\n",
    "\n",
    "Этот пункт состоит из двух этапов: сначала надо обучить языковую модель, затем встроить ее в beam search.\n",
    "\n",
    "#### Обучение\n",
    "\n",
    "Для обучения n-gram языковой модели можно использовать фреймворк kenlm.\n",
    "Документация: \n",
    "* https://kheafield.com/code/kenlm/\n",
    "* https://github.com/kpu/kenlm\n",
    "\n",
    "Модель сначала строится в формате arpa, затем ее лучше перевести в формат trie. Вызывать полученную модель можно через питон [ссылка](https://github.com/kpu/kenlm#python-module)\n",
    "\n",
    "\n",
    "#### Данные\n",
    "\n",
    "Для обучения модели конечно нужны данные :) Тут есть варианты:\n",
    "\n",
    "1. Можно обучить маленькую языковую модель на текстах из акустических обучающих данных (из трейна!).\n",
    "\n",
    "минусы: этих данных мало\n",
    "\n",
    "плюсы: домен остается таким же\n",
    "\n",
    "2. Можно взять внешние данные, например, [отсюда](http://data.statmt.org/cc-100/). (46G).\n",
    "\n",
    "минусы: тексты из другого (произвольного) домена\n",
    "\n",
    "плюсы: данных много\n",
    "\n",
    "При необходимости, данные надо предобработать - привести к нижнему регистру, разделить на предложения. Убрать предложения, которые содержат символы не из русского алфавита \n",
    "\n",
    "\n",
    "#### Внедрение в beam search\n",
    "\n",
    "В разделе про beam search есть ссылки на алгоритм.\n",
    "\n",
    "Можно использовать аргумент `ext_scoring_func` [тут](https://github.com/PaddlePaddle/DeepSpeech/blob/master/decoders/decoders_deprecated.py#L47).\n",
    "\n",
    "Пример скорера можно найти [тут](https://github.com/PaddlePaddle/DeepSpeech/blob/master/decoders/scorer_deprecated.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментации ★\n",
    "\n",
    "\n",
    "Позволяют искусственно увеличь размер обучающей выборки, сделать его более разнообразным.\n",
    "\n",
    "### Аугментации аудио \n",
    "\n",
    "Применяются к аудиосигналу. Аугментации обычно реализуются через [sox](https://ru.wikipedia.org/wiki/SoX_(%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0)). [Тут](http://sox.sourceforge.net/sox.html#EFFECTS) можно посмотротреть полный список sox эффектов с описанием. \n",
    "\n",
    "Полный список sox эффектов, доступных в torchaudio, можно посмотреть [тут](https://github.com/pytorch/audio/issues/260).\n",
    "\n",
    "Эффекты можно комбинировать.\n",
    "\n",
    "Внимание!\n",
    "\n",
    "* Аугментации надо применять очень аккуратно (!) - слишком сильные аугментации только ухудшат качество. Лучше применять аугментации с некоторой вероятностью.\n",
    "* Применять **только на обучающую выборку, не на валидацию!**\n",
    "* Некоторые аугментации меняют sample rate и длину аудио.\n",
    "* Можно применять не открывая предварительно аудиофайл [ссылка](https://pytorch.org/audio/stable/sox_effects.html#applying-effects-on-file).\n",
    "\n",
    "\n",
    "Примеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio, sample_rate, effects):\n",
    "    \n",
    "    effects = [effects, ['rate', '8000']]\n",
    "\n",
    "    augmented_audio, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
    "        torch.unsqueeze(audio, 0),\n",
    "        sample_rate=sample_rate,\n",
    "        effects=effects,\n",
    "        channels_first=True)\n",
    "    \n",
    "    return augmented_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000\n",
    "audio, audio_len = open_audio('test_files/test_audio.mp3', sample_rate)\n",
    "Audio(data=audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['treble', '20'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['bass', '20'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['pitch', '400'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['speed', '1.5'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = augment_audio(audio, sample_rate, ['tempo', '1.5'])\n",
    "Audio(data=augmented_audio.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментации спектрограммы\n",
    "\n",
    "[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/pdf/1904.08779.pdf)\n",
    "\n",
    "Методов [Frequency masking](https://pytorch.org/audio/stable/transforms.html#frequencymasking) и [Time masking](https://pytorch.org/audio/stable/transforms.html#timemasking) должно быть достаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что еще можно попробовать: \n",
    "\n",
    "1. Поэкспериментировать с learning rate, оптимайзером (например, взять SGD). Можно добавить [lr decay](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ExponentialLR) ★\n",
    "2. Поэкспериментировать с нейронной сетью и восттановлением из чекпоинта. Например, зафиксировать предобученные слои и дообучить остальные, потом с маленьким learning rate дообучить всю модель. ★\n",
    "3. Добавить новые слои в нейронную сеть. ★★\n",
    "4. Использовать больше данных. ★★★\n",
    "<img src=\"images/more_data.jpg\" width=\"400\" height=\"400\">\n",
    "\n",
    "В этом случае для ускорения можно запустить распределенное обучение на нескольких gpu с помощью [horovod](https://github.com/horovod/horovod). Данные можно взять тут:\n",
    "\n",
    "* [open_stt](https://github.com/snakers4/open_stt) (до 2.5 TB данных!)\n",
    "* [Russian LibriSpeech](https://openslr.org/96/) (9 GB данных)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Не использовать чекпоинт и обучить свою сеть :) ★ (потребуется больше данных!)\n",
    "\n",
    "6\\. Shallow fusion - можно обучить дополнительную языковую модель и использовать в качестве рескорера \n",
    "[ссылка](https://arxiv.org/pdf/1503.03535.pdf). ★\n",
    "\n",
    "7\\. Реализовать перемешивание батчей. ★\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
